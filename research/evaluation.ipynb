{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9cc0853",
   "metadata": {},
   "source": [
    "Оценка RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import numpy as np\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, ResponseRelevancy\n",
    "import os\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "AGENT_HOST = os.getenv(\"AGENT_HOST\", \"http://localhost\")\n",
    "AGENT_PORT = int(os.getenv(\"AGENT_PORT\", 2024))\n",
    "AGENT_API_KEY = os.getenv(\"AGENT_API_KEY\")\n",
    "AGENT_NAME = os.getenv(\"AGENT_NAME\", \"agent\")\n",
    "\n",
    "from langgraph_sdk import get_client \n",
    "\n",
    "client = get_client(url=AGENT_HOST+\":\"+str(AGENT_PORT), api_key=AGENT_API_KEY)\n",
    "\n",
    "async def answer_to_user_func(context: str) -> str:\n",
    "        \n",
    "    assistants = await client.assistants.search()\n",
    "    assistants = await client.assistants.search(graph_id=AGENT_NAME)\n",
    "    \n",
    "    agent = assistants[0]\n",
    "    thread = await client.threads.create()\n",
    "    \n",
    "    user_text = context\n",
    "    if not user_text:\n",
    "        return \"❗ Пустой запрос\"\n",
    "    human = HumanMessage(content=user_text)\n",
    "    payload = {\"messages\": [human]}\n",
    "\n",
    "    try:\n",
    "        async for response_obj in client.runs.stream(thread['thread_id'], agent['assistant_id'], input=payload, config={\"recursion_limit\": 100}):\n",
    "            print(response_obj)\n",
    "        \n",
    "        msgs = response_obj.data['messages'][-1][\"content\"]\n",
    "            \n",
    "        if not msgs:\n",
    "            return \"Ошибка при ответе, праститеееее исправимся, дайте деняк по больше токенов и сооооон\"\n",
    "        \n",
    "        return msgs\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Ошибка при обращении к агенту: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ce98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, ResponseRelevancy\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import numpy as np\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import  Faithfulness, ResponseRelevancy,ContextPrecision, ContextRecall, AnswerCorrectness\n",
    "\n",
    "MAX_TOKENS = 2048 * 1\n",
    "from functools import lru_cache\n",
    "from typing import Any, Callable, List\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from react_agent.configuration import Configuration\n",
    "\n",
    "from qdrant_client import QdrantClient, models\n",
    "from fastembed import SparseTextEmbedding\n",
    "from yandex_cloud_ml_sdk import YCloudML\n",
    "\n",
    "# Qdrant + Embedding SDK\n",
    "qclient = QdrantClient(os.environ[\"QDRANT_URL\"], api_key=os.environ[\"QDRANT_API_KEY\"])\n",
    "sdk = YCloudML(folder_id=os.environ[\"FOLDER_ID\"], auth=os.environ[\"API_KEY\"])\n",
    "\n",
    "dense_query_embedding_model = sdk.models.text_embeddings(\"query\")\n",
    "dense_doc_embedding_model = sdk.models.text_embeddings(\"doc\")\n",
    "bm25_embedding_model = SparseTextEmbedding(\"Qdrant/bm25\")\n",
    "\n",
    "model = sdk.models.completions(\"yandexgpt\", model_version=\"rc\")\n",
    "with open('/home/ivzarru/mathmod2025/Yandex-multimodal-assistant/src/react_agent/data/tagging_prompt.md', 'r', encoding='utf-8') as f:\n",
    "    TAGGING_PROMPT = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a087b0",
   "metadata": {},
   "source": [
    "Создание датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292aadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langgraph_sdk import get_client\n",
    "import qdrant_client.models as models\n",
    "\n",
    "# Импорт функции ответов агентного API\n",
    "import os\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "AGENT_HOST = os.getenv(\"AGENT_HOST\", \"http://localhost\")\n",
    "AGENT_PORT = int(os.getenv(\"AGENT_PORT\", 2024))\n",
    "AGENT_API_KEY = os.getenv(\"AGENT_API_KEY\")\n",
    "AGENT_NAME = os.getenv(\"AGENT_NAME\", \"agent\")\n",
    "\n",
    "from langgraph_sdk import get_client \n",
    "\n",
    "client = get_client(url=AGENT_HOST+\":\"+str(AGENT_PORT), api_key=AGENT_API_KEY)\n",
    "\n",
    "async def answer_to_user_func(context: str) -> str:\n",
    "        \n",
    "    assistants = await client.assistants.search()\n",
    "    assistants = await client.assistants.search(graph_id=AGENT_NAME)\n",
    "    \n",
    "    agent = assistants[0]\n",
    "    thread = await client.threads.create()\n",
    "    \n",
    "    user_text = context\n",
    "    if not user_text:\n",
    "        return \"❗ Пустой запрос\"\n",
    "    human = HumanMessage(content=user_text)\n",
    "    payload = {\"messages\": [human]}\n",
    "\n",
    "    try:\n",
    "        async for response_obj in client.runs.stream(thread['thread_id'], agent['assistant_id'], input=payload, config={\"recursion_limit\": 100}):\n",
    "            print(response_obj)\n",
    "        \n",
    "        msgs = response_obj.data['messages'][-1][\"content\"]\n",
    "            \n",
    "        if not msgs:\n",
    "            return \"Ошибка при ответе, праститеееее исправимся, дайте деняк по больше токенов и сооооон\"\n",
    "        \n",
    "        return msgs\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Ошибка при обращении к агенту: {e}\"\n",
    "\n",
    "# Импорт для оценки\n",
    "from yandex_chain import YandexEmbeddings\n",
    "\n",
    "\n",
    "# Загрузка переменных окружения\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# ----- Настройка LLM для тегирования и эмбеддингов -----\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt://\" + os.environ[\"FOLDER_ID\"] + '/yandexgpt/rc',\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    "    temperature=0.1,\n",
    "    base_url=\"https://llm.api.cloud.yandex.net/v1\"\n",
    ")\n",
    "\n",
    "# === Инициализируйте свои модели эмбеддингов и Qdrant клиент ===\n",
    "# dense_query_embedding_model = ...\n",
    "# bm25_embedding_model = ...\n",
    "# qclient = ...\n",
    "# TAGGING_PROMPT = \"...\"\n",
    "# MAX_TOKENS = 1024\n",
    "\n",
    "# Функция для тегирования запроса\n",
    "def tag_query(query: str) -> str:\n",
    "    return llm.invoke(input=TAGGING_PROMPT + \"\\n\\n Входной запрос:\\n\" + query).content\n",
    "\n",
    "# Функция поиска по коллекции Qdrant\n",
    "def query_from_collection(query: str, collection_name: str, top_k: int = 5) -> str:\n",
    "    dense_query_vector = dense_query_embedding_model.run(query[:MAX_TOKENS*2]).embedding\n",
    "    sparse_query_vector = list(bm25_embedding_model.embed([query]))[0]\n",
    "    query_topics = set(tag_query(query).split(', '))\n",
    "    query_topics.discard('мусор')\n",
    "\n",
    "    prefetch = [\n",
    "        models.Prefetch(query=dense_query_vector, using=\"dense\", limit=6*top_k),\n",
    "        models.Prefetch(query=models.SparseVector(**sparse_query_vector.as_object()), using=\"sparse\", limit=6*top_k),\n",
    "    ]\n",
    "\n",
    "    resp = qclient.query_points(\n",
    "        collection_name,\n",
    "        prefetch=prefetch,\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        with_payload=True,\n",
    "        limit=7*top_k,\n",
    "    )\n",
    "    threshold = sum(p.score for p in resp.points) / len(resp.points)\n",
    "\n",
    "    results = []\n",
    "    for point in resp.points:\n",
    "        source = point.payload['source']\n",
    "        if 'chat' in source:\n",
    "            date = source.split('_')[0].replace('chat', '')\n",
    "        else:\n",
    "            date = {\n",
    "                'Онлайн_магистратура_«Машинное_обучение_и_анализ_данных»': 2024,\n",
    "                'Особые_условия_для_поступления_в_Институт_№8_МАИ': '2024-2026',\n",
    "                'Постановление Правительства РФ от 27.04.2024 N 555 — Редакция от 07.04.2025 — Контур.Норматив': '2025-2026',\n",
    "                'Правила приема МАИ': '2025-2026',\n",
    "                'Правила Приема(Министерские)': '2024 - 2026',\n",
    "                'Федеральный закон от 29 декабря 2012 г. N 273-ФЗ _Об образовании в Российской Фе ... _ Система ГАРАНТ': '2012 - 2025'\n",
    "            }.get(source, '2025-2026')\n",
    "\n",
    "        if set(point.payload['topics']).intersection(query_topics) or point.score > threshold:\n",
    "            results.append(f\"Актуально в период: {date}\\n\\n{point.payload['chunk_text']}\")\n",
    "\n",
    "    results = results[:top_k]\n",
    "    return \"\\n\".join([\n",
    "        \"=\" * 20 + f\"\\n# Источник номер {i + 1}\\n\" + text\n",
    "        for i, text in enumerate(results)\n",
    "    ])\n",
    "\n",
    "# Функция получения списком отрывков контекста\n",
    "def retrieve(query: str, top_k: int = 10) -> list[str]:\n",
    "    raw = query_from_collection(query, \"Collection_pdf\", top_k=top_k)\n",
    "    parts = [p.strip() for p in raw.split(\"=\" * 20) if p.strip()]\n",
    "    return parts[:top_k]\n",
    "\n",
    "# Главная асинхронная функция\n",
    "async def main():\n",
    "    # Чтение входных данных\n",
    "    questions = open('../data/testing_data/test_cases.txt', encoding='utf-8').read().splitlines()\n",
    "    gt = open('../data/testing_data/ground_truth.txt', 'r', encoding='utf-8').read().splitlines('$$$')\n",
    "\n",
    "    dataset = []\n",
    "    mew = []\n",
    "    # Загрузка уже существующих ответов\n",
    "    try:\n",
    "        with open('mew.json', 'r', encoding='utf-8') as f:\n",
    "            mew = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        mew = []\n",
    "\n",
    "    c = 0\n",
    "    for q in tqdm(questions, desc=\"Обработка вопросов\", unit=\"вопрос\"):\n",
    "        # Проверяем, есть ли уже такой вопрос\n",
    "        existing = next((e for e in mew if e['user_input'] == q), None)\n",
    "        if existing:\n",
    "            existing['reference'] = gt[c]\n",
    "            dataset.append(existing)\n",
    "            c += 1\n",
    "            continue\n",
    "\n",
    "        # Получаем контекст и формируем вход для агента\n",
    "        contexts = retrieve(q)\n",
    "        prompt = f\"\"\"\n",
    "\n",
    "🎓 **AI-ассистент приёмной комиссии Московского авиационного института (МАИ)**\n",
    "\n",
    "🧑‍💼 **Твоя роль**  \n",
    "Ты — умный, доброжелательный и чёткий AI-помощник.  \n",
    "Твоя задача — сделать процесс поступления в МАИ **максимально простым и понятным** для каждого абитуриента.\n",
    "\n",
    "💬 **Как ты общаешься**\n",
    "- **Просто** — как будто объясняешь другу.\n",
    "- **По делу** — ничего лишнего, только нужная информация.\n",
    "- **С заботой** — чтобы поступающий чувствовал поддержку.\n",
    "\n",
    "📌 **Пример ответа:**  \n",
    "`Чтобы подать документы в МАИ онлайн, зайдите в личный кабинет на сайте приёмной комиссии. Вам понадобятся сканы паспорта, аттестата и СНИЛС.`\n",
    "\n",
    "🔍 **Как ты работаешь (логика диалога)**  \n",
    "Твой принцип: **думай → ищи → анализируй → помогай → проверяй → уточняй**\n",
    "\n",
    "1. **Каждый новый вопрос = новая гипотеза**\n",
    "    - Определи, чего хочет абитуриент.\n",
    "    - Понял? Сделай уточнение — даже если уверен.\n",
    "    - Не понял? Мягко уточни.\n",
    "\n",
    "2. **Всегда запускай ИНСТРУМЕНТЫ:**\n",
    "    - Сначала: `search` и `yandex_generative_search` (одновременно).\n",
    "    - Потом, по ситуации, повтори вызовы с уточнённым запросом.\n",
    "    - Меняй формулировки, ищи другие форматы ответа.\n",
    "    - Можно вызывать инструменты несколько раз — это правильно.\n",
    "\n",
    "3. **Обязательно АНАЛИЗИРУЙ каждый результат:**\n",
    "    - Что из найденного реально помогает?  \n",
    "    - Есть ли в нём дедлайны, баллы, нюансы?\n",
    "\n",
    "4. **Формируй ответ просто и точно**\n",
    "    - Со ссылками или с прямым действием.\n",
    "    - Если есть дедлайны — предупреди.\n",
    "    - Если есть опасность ошибки — подчеркни её.\n",
    "    - Если вопрос сложный — веди пошагово: сначала одно, потом следующее.\n",
    "\n",
    "5. **Если после 2–3 поисков нет результата — не тяни:**\n",
    "    - Предложи обратиться к оператору:\n",
    "    `По этому вопросу лучше уточнить в приёмной комиссии МАИ напрямую.`\n",
    "\n",
    "📌 **Всегда уточняй:**\n",
    "- Бакалавриат или магистратура?\n",
    "- Российский или иностранный абитуриент?\n",
    "- На платное или бюджетное?\n",
    "\n",
    "📅 **Контекст времени**\n",
    "— Помни про актуальность данных. Уточняй год, если вопрос может касаться прошлых лет.  \n",
    "— Учитывай, что информация может меняться — проверяй через инструменты.\n",
    "\n",
    "\n",
    "📥 **Онлайн-подача**\n",
    "Через Госуслуги или личный кабинет МАИ. Нужны: паспорт, аттестат, СНИЛС.  \n",
    "\n",
    "📢 **Согласие на зачисление**\n",
    "Не забудь: приоритет за теми, кто подал оригинал и согласие.\n",
    "\n",
    "🧠 **Твои главные принципы**\n",
    "✅ **Ты ведёшь диалог до результата**  \n",
    "✅ **Ты защищаешь абитуриента от ошибок**  \n",
    "✅ **Ты делаешь сложное простым**\n",
    "✅ **Будь проактивен. Предлагай, веди беседу**\n",
    "\n",
    "📞 **Что делать, если не знаешь ответа?**\n",
    "1. Сначала — **поиск (search + yandex_generative_search)**  \n",
    "2. Потом — **анализируй** и снова ищи, если нужно.  \n",
    "3. Только если совсем нет данных — скажи:  \n",
    "`К сожалению, сейчас я не могу найти точной информации. Рекомендую связаться с приёмной комиссией МАИ напрямую.`\n",
    "\n",
    "🎯 **Твоя миссия**\n",
    "Сделать так, чтобы любой человек — независимо от подготовки — мог поступить в МАИ спокойно и уверенно.  \n",
    "Ты — как личный наставник, который всегда рядом.\n",
    "\n",
    "\n",
    "**ОТВЕЧАЙ ПОЛЬЗОВАТЕЛЮ НА ЯЗЫКЕ, КОТОРЫЙ ОН ИСПОЛЬЗУЕТ. **\n",
    "\n",
    "Запрос пользователя:{q}\n",
    "\"\"\"\n",
    "\n",
    "        # Получаем ответ от агентного API\n",
    "        response_text = await answer_to_user_func(prompt)\n",
    "        if not response_text:\n",
    "            response_text = \"❗ Агент вернул пустой ответ\"\n",
    "\n",
    "        entry = {\n",
    "            \"user_input\": q,\n",
    "            \"retrieved_contexts\": contexts,\n",
    "            \"reference\": gt[c],\n",
    "            \"response\": response_text\n",
    "        }\n",
    "        c += 1\n",
    "        dataset.append(entry)\n",
    "        mew.append(entry)\n",
    "\n",
    "        # Сохраняем промежуточный файл\n",
    "        with open('mew.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(mew, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Сохраняем полный датасет\n",
    "    with open('dataset.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Подготавливаем к оценке\n",
    "    eval_dataset = EvaluationDataset.from_list(dataset)\n",
    "\n",
    "    evaluator_llm = ChatOpenAI(\n",
    "        model=\"gpt://\" + os.environ[\"FOLDER_ID\"] + '/yandexgpt/rc',\n",
    "        api_key=os.environ[\"API_KEY\"],\n",
    "        temperature=0,\n",
    "        base_url=\"https://llm.api.cloud.yandex.net/v1\"\n",
    "    )\n",
    "    embeddings = YandexEmbeddings(\n",
    "        api_key=os.environ[\"API_KEY\"],\n",
    "        folder_id=os.environ[\"FOLDER_ID\"],\n",
    "        model_version=\"rc\",\n",
    "        retries=10\n",
    "    )\n",
    "    evaluator = LangchainLLMWrapper(evaluator_llm)\n",
    "\n",
    "    metrics = [AnswerCorrectness()]\n",
    "\n",
    "    # Оценка ответов агента\n",
    "    results = evaluate(dataset=eval_dataset, metrics=metrics, llm=evaluator, embeddings=embeddings)\n",
    "    print(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572807f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "dataset = json.load(open('dataset.json', 'r', encoding='utf-8'))\n",
    "eval_ds = EvaluationDataset.from_list(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3c48c",
   "metadata": {},
   "source": [
    "Оценка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21e3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install yandex-chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e94f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yandex_chain import YandexEmbeddings\n",
    "\n",
    "embeddings = YandexEmbeddings(\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    "    folder_id=os.environ[\"FOLDER_ID\"],\n",
    "    model_version=\"rc\"\n",
    ")\n",
    "\n",
    "\n",
    "embeddings.embed_documents([\"Привет, мир!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88678d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import  Faithfulness, ResponseRelevancy\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import shutil  # Импортируем shutil для очистки папки\n",
    "\n",
    "# Загрузка переменных окружения\n",
    "load_dotenv()\n",
    "with open('dataset.json', 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "eval_dataset = EvaluationDataset.from_list(dataset)\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt://\" + os.environ[\"FOLDER_ID\"] + '/yandexgpt/rc',\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    "    temperature=0.1,\n",
    "    base_url=\"https://llm.api.cloud.yandex.net/v1\"\n",
    ")\n",
    "evaluator = LangchainLLMWrapper(llm)\n",
    "metrics = [\n",
    "    Faithfulness(),\n",
    "    ResponseRelevancy()\n",
    "]\n",
    "results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=evaluator,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e423da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt://\" + os.environ[\"FOLDER_ID\"] + '/yandexgpt/rc',\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    "    temperature=0.1,\n",
    "    base_url=\"https://llm.api.cloud.yandex.net/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c10308",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"fdfd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719a3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
