{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9cc0853",
   "metadata": {},
   "source": [
    "Оценка RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import numpy as np\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, ResponseRelevancy\n",
    "import os\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "AGENT_HOST = os.getenv(\"AGENT_HOST\", \"http://localhost\")\n",
    "AGENT_PORT = int(os.getenv(\"AGENT_PORT\", 2024))\n",
    "AGENT_API_KEY = os.getenv(\"AGENT_API_KEY\")\n",
    "AGENT_NAME = os.getenv(\"AGENT_NAME\", \"agent\")\n",
    "\n",
    "from langgraph_sdk import get_client \n",
    "\n",
    "client = get_client(url=AGENT_HOST+\":\"+str(AGENT_PORT), api_key=AGENT_API_KEY)\n",
    "\n",
    "async def answer_to_user_func(context: str) -> str:\n",
    "        \n",
    "    assistants = await client.assistants.search()\n",
    "    assistants = await client.assistants.search(graph_id=AGENT_NAME)\n",
    "    \n",
    "    agent = assistants[0]\n",
    "    thread = await client.threads.create()\n",
    "    \n",
    "    user_text = context\n",
    "    if not user_text:\n",
    "        return \"❗ Пустой запрос\"\n",
    "    human = HumanMessage(content=user_text)\n",
    "    payload = {\"messages\": [human]}\n",
    "\n",
    "    try:\n",
    "        async for response_obj in client.runs.stream(thread['thread_id'], agent['assistant_id'], input=payload, config={\"recursion_limit\": 100}):\n",
    "            print(response_obj)\n",
    "        \n",
    "        msgs = response_obj.data['messages'][-1][\"content\"]\n",
    "            \n",
    "        if not msgs:\n",
    "            return \"Ошибка при ответе, праститеееее исправимся, дайте деняк по больше токенов и сооооон\"\n",
    "        \n",
    "        return msgs\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Ошибка при обращении к агенту: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ce98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, ResponseRelevancy\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import numpy as np\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import  Faithfulness, ResponseRelevancy,ContextPrecision, ContextRecall, AnswerCorrectness\n",
    "\n",
    "MAX_TOKENS = 2048 * 1\n",
    "from functools import lru_cache\n",
    "from typing import Any, Callable, List\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from react_agent.configuration import Configuration\n",
    "\n",
    "from qdrant_client import QdrantClient, models\n",
    "from fastembed import SparseTextEmbedding\n",
    "from yandex_cloud_ml_sdk import YCloudML\n",
    "\n",
    "# Qdrant + Embedding SDK\n",
    "qclient = QdrantClient(os.environ[\"QDRANT_URL\"], api_key=os.environ[\"QDRANT_API_KEY\"])\n",
    "sdk = YCloudML(folder_id=os.environ[\"FOLDER_ID\"], auth=os.environ[\"API_KEY\"])\n",
    "\n",
    "dense_query_embedding_model = sdk.models.text_embeddings(\"query\")\n",
    "dense_doc_embedding_model = sdk.models.text_embeddings(\"doc\")\n",
    "bm25_embedding_model = SparseTextEmbedding(\"Qdrant/bm25\")\n",
    "\n",
    "model = sdk.models.completions(\"yandexgpt\", model_version=\"rc\")\n",
    "with open('/home/ivzarru/mathmod2025/Yandex-multimodal-assistant/src/react_agent/src/react_agent/data/tagging_prompt.md', 'r', encoding='utf-8') as f:\n",
    "    TAGGING_PROMPT = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a087b0",
   "metadata": {},
   "source": [
    "Создание датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292aadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langgraph_sdk import get_client\n",
    "import qdrant_client.models as models\n",
    "\n",
    "# Импорт функции ответов агентного API\n",
    "import os\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "AGENT_HOST = os.getenv(\"AGENT_HOST\", \"http://localhost\")\n",
    "AGENT_PORT = int(os.getenv(\"AGENT_PORT\", 2024))\n",
    "AGENT_API_KEY = os.getenv(\"AGENT_API_KEY\")\n",
    "AGENT_NAME = os.getenv(\"AGENT_NAME\", \"agent\")\n",
    "\n",
    "from langgraph_sdk import get_client \n",
    "\n",
    "client = get_client(url=AGENT_HOST+\":\"+str(AGENT_PORT), api_key=AGENT_API_KEY)\n",
    "\n",
    "async def answer_to_user_func(context: str) -> str:\n",
    "        \n",
    "    assistants = await client.assistants.search()\n",
    "    assistants = await client.assistants.search(graph_id=AGENT_NAME)\n",
    "    \n",
    "    agent = assistants[0]\n",
    "    thread = await client.threads.create()\n",
    "    \n",
    "    user_text = context\n",
    "    if not user_text:\n",
    "        return \"❗ Пустой запрос\"\n",
    "    human = HumanMessage(content=user_text)\n",
    "    payload = {\"messages\": [human]}\n",
    "\n",
    "    try:\n",
    "        async for response_obj in client.runs.stream(thread['thread_id'], agent['assistant_id'], input=payload, config={\"recursion_limit\": 100}):\n",
    "            print(response_obj)\n",
    "        \n",
    "        msgs = response_obj.data['messages'][-1][\"content\"]\n",
    "            \n",
    "        if not msgs:\n",
    "            return \"Ошибка при ответе, праститеееее исправимся, дайте деняк по больше токенов и сооооон\"\n",
    "        \n",
    "        return msgs\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Ошибка при обращении к агенту: {e}\"\n",
    "\n",
    "# Импорт для оценки\n",
    "from yandex_chain import YandexEmbeddings\n",
    "\n",
    "\n",
    "# Загрузка переменных окружения\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# ----- Настройка LLM для тегирования и эмбеддингов -----\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt://\" + os.environ[\"FOLDER_ID\"] + '/yandexgpt/rc',\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    "    temperature=0.1,\n",
    "    base_url=\"https://llm.api.cloud.yandex.net/v1\"\n",
    ")\n",
    "\n",
    "# === Инициализируйте свои модели эмбеддингов и Qdrant клиент ===\n",
    "# dense_query_embedding_model = ...\n",
    "# bm25_embedding_model = ...\n",
    "# qclient = ...\n",
    "# TAGGING_PROMPT = \"...\"\n",
    "# MAX_TOKENS = 1024\n",
    "\n",
    "# Функция для тегирования запроса\n",
    "def tag_query(query: str) -> str:\n",
    "    return llm.invoke(input=TAGGING_PROMPT + \"\\n\\n Входной запрос:\\n\" + query).content\n",
    "\n",
    "# Функция поиска по коллекции Qdrant\n",
    "def query_from_collection(query: str, collection_name: str, top_k: int = 5) -> str:\n",
    "    dense_query_vector = dense_query_embedding_model.run(query[:MAX_TOKENS*2]).embedding\n",
    "    sparse_query_vector = list(bm25_embedding_model.embed([query]))[0]\n",
    "    query_topics = set(tag_query(query).split(', '))\n",
    "    query_topics.discard('мусор')\n",
    "\n",
    "    prefetch = [\n",
    "        models.Prefetch(query=dense_query_vector, using=\"dense\", limit=6*top_k),\n",
    "        models.Prefetch(query=models.SparseVector(**sparse_query_vector.as_object()), using=\"sparse\", limit=6*top_k),\n",
    "    ]\n",
    "\n",
    "    resp = qclient.query_points(\n",
    "        collection_name,\n",
    "        prefetch=prefetch,\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        with_payload=True,\n",
    "        limit=7*top_k,\n",
    "    )\n",
    "    threshold = sum(p.score for p in resp.points) / len(resp.points)\n",
    "\n",
    "    results = []\n",
    "    for point in resp.points:\n",
    "        source = point.payload['source']\n",
    "        if 'chat' in source:\n",
    "            date = source.split('_')[0].replace('chat', '')\n",
    "        else:\n",
    "            date = {\n",
    "                'Онлайн_магистратура_«Машинное_обучение_и_анализ_данных»': 2024,\n",
    "                'Особые_условия_для_поступления_в_Институт_№8_МАИ': '2024-2026',\n",
    "                'Постановление Правительства РФ от 27.04.2024 N 555 — Редакция от 07.04.2025 — Контур.Норматив': '2025-2026',\n",
    "                'Правила приема МАИ': '2025-2026',\n",
    "                'Правила Приема(Министерские)': '2024 - 2026',\n",
    "                'Федеральный закон от 29 декабря 2012 г. N 273-ФЗ _Об образовании в Российской Фе ... _ Система ГАРАНТ': '2012 - 2025'\n",
    "            }.get(source, '2025-2026')\n",
    "\n",
    "        if set(point.payload['topics']).intersection(query_topics) or point.score > threshold:\n",
    "            results.append(f\"Актуально в период: {date}\\n\\n{point.payload['chunk_text']}\")\n",
    "\n",
    "    results = results[:top_k]\n",
    "    return \"\\n\".join([\n",
    "        \"=\" * 20 + f\"\\n# Источник номер {i + 1}\\n\" + text\n",
    "        for i, text in enumerate(results)\n",
    "    ])\n",
    "\n",
    "# Функция получения списком отрывков контекста\n",
    "def retrieve(query: str, top_k: int = 10) -> list[str]:\n",
    "    raw = query_from_collection(query, \"Collection_pdf\", top_k=top_k)\n",
    "    parts = [p.strip() for p in raw.split(\"=\" * 20) if p.strip()]\n",
    "    return parts[:top_k]\n",
    "\n",
    "# Главная асинхронная функция\n",
    "async def main():\n",
    "    # Чтение входных данных\n",
    "    gt = open('../data/testing_data/ground_truth.txt', 'r', encoding='utf-8').read().splitlines('$$$')[:10]\n",
    "\n",
    "    questions = open('../data/testing_data/test_cases.txt', encoding='utf-8').read().splitlines()[:len(gt)]\n",
    "\n",
    "    dataset = []\n",
    "    mew = []\n",
    "    # Загрузка уже существующих ответов\n",
    "    try:\n",
    "        with open('mew.json', 'r', encoding='utf-8') as f:\n",
    "            mew = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        mew = []\n",
    "\n",
    "    c = 0\n",
    "    for q in tqdm(questions, desc=\"Обработка вопросов\", unit=\"вопрос\"):\n",
    "        # Проверяем, есть ли уже такой вопрос\n",
    "        existing = next((e for e in mew if e['user_input'] == q), None)\n",
    "        if existing:\n",
    "            existing['reference'] = gt[c]\n",
    "            dataset.append(existing)\n",
    "            c += 1\n",
    "            continue\n",
    "\n",
    "        # Получаем контекст и формируем вход для агента\n",
    "        contexts = retrieve(q)\n",
    "        prompt = q\n",
    "\n",
    "        # Получаем ответ от агентного API\n",
    "        response_text = await answer_to_user_func(prompt)\n",
    "        if not response_text:\n",
    "            response_text = \"❗ Агент вернул пустой ответ\"\n",
    "\n",
    "        entry = {\n",
    "            \"user_input\": q,\n",
    "            \"retrieved_contexts\": contexts,\n",
    "            \"reference\": gt[c],\n",
    "            \"response\": response_text\n",
    "        }\n",
    "        c += 1\n",
    "        dataset.append(entry)\n",
    "        mew.append(entry)\n",
    "\n",
    "        # Сохраняем промежуточный файл\n",
    "        with open('mew.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(mew, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Сохраняем полный датасет\n",
    "    with open('dataset.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Подготавливаем к оценке\n",
    "    eval_dataset = EvaluationDataset.from_list(dataset)\n",
    "\n",
    "    evaluator_llm = ChatOpenAI(\n",
    "        model=\"gpt://\" + os.environ[\"FOLDER_ID\"] + '/yandexgpt/rc',\n",
    "        api_key=os.environ[\"API_KEY\"],\n",
    "        temperature=0,\n",
    "        base_url=\"https://llm.api.cloud.yandex.net/v1\"\n",
    "    )\n",
    "    embeddings = YandexEmbeddings(\n",
    "        api_key=os.environ[\"API_KEY\"],\n",
    "        folder_id=os.environ[\"FOLDER_ID\"],\n",
    "        model_version=\"rc\",\n",
    "        retries=10\n",
    "    )\n",
    "    evaluator = LangchainLLMWrapper(evaluator_llm)\n",
    "\n",
    "    metrics = [AnswerCorrectness()]\n",
    "\n",
    "    # Оценка ответов агента\n",
    "    results = evaluate(dataset=eval_dataset, metrics=metrics, llm=evaluator, embeddings=embeddings)\n",
    "    print(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572807f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "dataset = json.load(open('dataset.json', 'r', encoding='utf-8'))\n",
    "eval_ds = EvaluationDataset.from_list(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3c48c",
   "metadata": {},
   "source": [
    "Оценка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21e3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install yandex-chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e94f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yandex_chain import YandexEmbeddings\n",
    "\n",
    "embeddings = YandexEmbeddings(\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    "    folder_id=os.environ[\"FOLDER_ID\"],\n",
    "    model_version=\"rc\"\n",
    ")\n",
    "\n",
    "\n",
    "embeddings.embed_documents([\"Привет, мир!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88678d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import  Faithfulness, ResponseRelevancy\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import shutil  # Импортируем shutil для очистки папки\n",
    "\n",
    "# Загрузка переменных окружения\n",
    "load_dotenv()\n",
    "with open('dataset.json', 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "eval_dataset = EvaluationDataset.from_list(dataset)\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt://\" + os.environ[\"FOLDER_ID\"] + '/yandexgpt/rc',\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    "    temperature=0.1,\n",
    "    base_url=\"https://llm.api.cloud.yandex.net/v1\"\n",
    ")\n",
    "evaluator = LangchainLLMWrapper(llm)\n",
    "metrics = [\n",
    "    Faithfulness(),\n",
    "    ResponseRelevancy()\n",
    "]\n",
    "results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=evaluator,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e423da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt://\" + os.environ[\"FOLDER_ID\"] + '/yandexgpt/rc',\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    "    temperature=0.1,\n",
    "    base_url=\"https://llm.api.cloud.yandex.net/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c10308",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"fdfd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719a3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
