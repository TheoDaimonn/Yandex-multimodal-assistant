{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9cc0853",
   "metadata": {},
   "source": [
    "–û—Ü–µ–Ω–∫–∞ RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import numpy as np\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, ResponseRelevancy\n",
    "import os\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "AGENT_HOST = os.getenv(\"AGENT_HOST\", \"http://localhost\")\n",
    "AGENT_PORT = int(os.getenv(\"AGENT_PORT\", 2024))\n",
    "AGENT_API_KEY = os.getenv(\"AGENT_API_KEY\")\n",
    "AGENT_NAME = os.getenv(\"AGENT_NAME\", \"agent\")\n",
    "\n",
    "from langgraph_sdk import get_client \n",
    "\n",
    "client = get_client(url=AGENT_HOST+\":\"+str(AGENT_PORT), api_key=AGENT_API_KEY)\n",
    "\n",
    "async def answer_to_user_func(context: str) -> str:\n",
    "        \n",
    "    assistants = await client.assistants.search()\n",
    "    assistants = await client.assistants.search(graph_id=AGENT_NAME)\n",
    "    \n",
    "    agent = assistants[0]\n",
    "    thread = await client.threads.create()\n",
    "    \n",
    "    user_text = context\n",
    "    if not user_text:\n",
    "        return \"‚ùó –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å\"\n",
    "    human = HumanMessage(content=user_text)\n",
    "    payload = {\"messages\": [human]}\n",
    "\n",
    "    try:\n",
    "        async for response_obj in client.runs.stream(thread['thread_id'], agent['assistant_id'], input=payload, config={\"recursion_limit\": 100}):\n",
    "            print(response_obj)\n",
    "        \n",
    "        msgs = response_obj.data['messages'][-1][\"content\"]\n",
    "            \n",
    "        if not msgs:\n",
    "            return \"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ, –ø—Ä–∞—Å—Ç–∏—Ç–µ–µ–µ–µ–µ –∏—Å–ø—Ä–∞–≤–∏–º—Å—è, –¥–∞–π—Ç–µ –¥–µ–Ω—è–∫ –ø–æ –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Å–æ–æ–æ–æ–æ–Ω\"\n",
    "        \n",
    "        return msgs\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –∞–≥–µ–Ω—Ç—É: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ce98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, ResponseRelevancy\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import numpy as np\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import  Faithfulness, ResponseRelevancy,ContextPrecision, ContextRecall, AnswerCorrectness\n",
    "\n",
    "MAX_TOKENS = 2048 * 1\n",
    "from functools import lru_cache\n",
    "from typing import Any, Callable, List\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from react_agent.configuration import Configuration\n",
    "\n",
    "from qdrant_client import QdrantClient, models\n",
    "from fastembed import SparseTextEmbedding\n",
    "from yandex_cloud_ml_sdk import YCloudML\n",
    "\n",
    "# Qdrant + Embedding SDK\n",
    "qclient = QdrantClient(os.environ[\"QDRANT_URL\"], api_key=os.environ[\"QDRANT_API_KEY\"])\n",
    "sdk = YCloudML(folder_id=os.environ[\"FOLDER_ID\"], auth=os.environ[\"API_KEY\"])\n",
    "\n",
    "dense_query_embedding_model = sdk.models.text_embeddings(\"query\")\n",
    "dense_doc_embedding_model = sdk.models.text_embeddings(\"doc\")\n",
    "bm25_embedding_model = SparseTextEmbedding(\"Qdrant/bm25\")\n",
    "\n",
    "model = sdk.models.completions(\"yandexgpt\", model_version=\"rc\")\n",
    "with open('/home/ivzarru/mathmod2025/Yandex-multimodal-assistant/src/react_agent/data/tagging_prompt.md', 'r', encoding='utf-8') as f:\n",
    "    TAGGING_PROMPT = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a087b0",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292aadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langgraph_sdk import get_client\n",
    "import qdrant_client.models as models\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ API\n",
    "import os\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "AGENT_HOST = os.getenv(\"AGENT_HOST\", \"http://localhost\")\n",
    "AGENT_PORT = int(os.getenv(\"AGENT_PORT\", 2024))\n",
    "AGENT_API_KEY = os.getenv(\"AGENT_API_KEY\")\n",
    "AGENT_NAME = os.getenv(\"AGENT_NAME\", \"agent\")\n",
    "\n",
    "from langgraph_sdk import get_client \n",
    "\n",
    "client = get_client(url=AGENT_HOST+\":\"+str(AGENT_PORT), api_key=AGENT_API_KEY)\n",
    "\n",
    "async def answer_to_user_func(context: str) -> str:\n",
    "        \n",
    "    assistants = await client.assistants.search()\n",
    "    assistants = await client.assistants.search(graph_id=AGENT_NAME)\n",
    "    \n",
    "    agent = assistants[0]\n",
    "    thread = await client.threads.create()\n",
    "    \n",
    "    user_text = context\n",
    "    if not user_text:\n",
    "        return \"‚ùó –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å\"\n",
    "    human = HumanMessage(content=user_text)\n",
    "    payload = {\"messages\": [human]}\n",
    "\n",
    "    try:\n",
    "        async for response_obj in client.runs.stream(thread['thread_id'], agent['assistant_id'], input=payload, config={\"recursion_limit\": 100}):\n",
    "            print(response_obj)\n",
    "        \n",
    "        msgs = response_obj.data['messages'][-1][\"content\"]\n",
    "            \n",
    "        if not msgs:\n",
    "            return \"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ, –ø—Ä–∞—Å—Ç–∏—Ç–µ–µ–µ–µ–µ –∏—Å–ø—Ä–∞–≤–∏–º—Å—è, –¥–∞–π—Ç–µ –¥–µ–Ω—è–∫ –ø–æ –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Å–æ–æ–æ–æ–æ–Ω\"\n",
    "        \n",
    "        return msgs\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –∞–≥–µ–Ω—Ç—É: {e}\"\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏\n",
    "from yandex_chain import YandexEmbeddings\n",
    "\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# ----- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLM –¥–ª—è —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ -----\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt://\" + os.environ[\"FOLDER_ID\"] + '/yandexgpt/rc',\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    "    temperature=0.1,\n",
    "    base_url=\"https://llm.api.cloud.yandex.net/v1\"\n",
    ")\n",
    "\n",
    "# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å–≤–æ–∏ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ Qdrant –∫–ª–∏–µ–Ω—Ç ===\n",
    "# dense_query_embedding_model = ...\n",
    "# bm25_embedding_model = ...\n",
    "# qclient = ...\n",
    "# TAGGING_PROMPT = \"...\"\n",
    "# MAX_TOKENS = 1024\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞\n",
    "def tag_query(query: str) -> str:\n",
    "    return llm.invoke(input=TAGGING_PROMPT + \"\\n\\n –í—Ö–æ–¥–Ω–æ–π –∑–∞–ø—Ä–æ—Å:\\n\" + query).content\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –ø–æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant\n",
    "def query_from_collection(query: str, collection_name: str, top_k: int = 5) -> str:\n",
    "    dense_query_vector = dense_query_embedding_model.run(query[:MAX_TOKENS*2]).embedding\n",
    "    sparse_query_vector = list(bm25_embedding_model.embed([query]))[0]\n",
    "    query_topics = set(tag_query(query).split(', '))\n",
    "    query_topics.discard('–º—É—Å–æ—Ä')\n",
    "\n",
    "    prefetch = [\n",
    "        models.Prefetch(query=dense_query_vector, using=\"dense\", limit=6*top_k),\n",
    "        models.Prefetch(query=models.SparseVector(**sparse_query_vector.as_object()), using=\"sparse\", limit=6*top_k),\n",
    "    ]\n",
    "\n",
    "    resp = qclient.query_points(\n",
    "        collection_name,\n",
    "        prefetch=prefetch,\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        with_payload=True,\n",
    "        limit=7*top_k,\n",
    "    )\n",
    "    threshold = sum(p.score for p in resp.points) / len(resp.points)\n",
    "\n",
    "    results = []\n",
    "    for point in resp.points:\n",
    "        source = point.payload['source']\n",
    "        if 'chat' in source:\n",
    "            date = source.split('_')[0].replace('chat', '')\n",
    "        else:\n",
    "            date = {\n",
    "                '–û–Ω–ª–∞–π–Ω_–º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä–∞_¬´–ú–∞—à–∏–Ω–Ω–æ–µ_–æ–±—É—á–µ–Ω–∏–µ_–∏_–∞–Ω–∞–ª–∏–∑_–¥–∞–Ω–Ω—ã—Ö¬ª': 2024,\n",
    "                '–û—Å–æ–±—ã–µ_—É—Å–ª–æ–≤–∏—è_–¥–ª—è_–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è_–≤_–ò–Ω—Å—Ç–∏—Ç—É—Ç_‚Ññ8_–ú–ê–ò': '2024-2026',\n",
    "                '–ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –†–§ –æ—Ç 27.04.2024 N 555 ‚Äî –†–µ–¥–∞–∫—Ü–∏—è –æ—Ç 07.04.2025 ‚Äî –ö–æ–Ω—Ç—É—Ä.–ù–æ—Ä–º–∞—Ç–∏–≤': '2025-2026',\n",
    "                '–ü—Ä–∞–≤–∏–ª–∞ –ø—Ä–∏–µ–º–∞ –ú–ê–ò': '2025-2026',\n",
    "                '–ü—Ä–∞–≤–∏–ª–∞ –ü—Ä–∏–µ–º–∞(–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å–∫–∏–µ)': '2024 - 2026',\n",
    "                '–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–∏ÃÜ –∑–∞–∫–æ–Ω –æ—Ç 29 –¥–µ–∫–∞–±—Ä—è 2012 –≥. N 273-–§–ó _–û–± –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ –≤ –†–æ—Å—Å–∏–∏ÃÜ—Å–∫–æ–∏ÃÜ –§–µ ... _ –°–∏—Å—Ç–µ–º–∞ –ì–ê–†–ê–ù–¢': '2012 - 2025'\n",
    "            }.get(source, '2025-2026')\n",
    "\n",
    "        if set(point.payload['topics']).intersection(query_topics) or point.score > threshold:\n",
    "            results.append(f\"–ê–∫—Ç—É–∞–ª—å–Ω–æ –≤ –ø–µ—Ä–∏–æ–¥: {date}\\n\\n{point.payload['chunk_text']}\")\n",
    "\n",
    "    results = results[:top_k]\n",
    "    return \"\\n\".join([\n",
    "        \"=\" * 20 + f\"\\n# –ò—Å—Ç–æ—á–Ω–∏–∫ –Ω–æ–º–µ—Ä {i + 1}\\n\" + text\n",
    "        for i, text in enumerate(results)\n",
    "    ])\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–æ–º –æ—Ç—Ä—ã–≤–∫–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "def retrieve(query: str, top_k: int = 10) -> list[str]:\n",
    "    raw = query_from_collection(query, \"Collection_pdf\", top_k=top_k)\n",
    "    parts = [p.strip() for p in raw.split(\"=\" * 20) if p.strip()]\n",
    "    return parts[:top_k]\n",
    "\n",
    "# –ì–ª–∞–≤–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è\n",
    "async def main():\n",
    "    # –ß—Ç–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    questions = open('../data/testing_data/test_cases.txt', encoding='utf-8').read().splitlines()\n",
    "    gt = open('../data/testing_data/ground_truth.txt', 'r', encoding='utf-8').read().splitlines('$$$')\n",
    "\n",
    "    dataset = []\n",
    "    mew = []\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤\n",
    "    try:\n",
    "        with open('mew.json', 'r', encoding='utf-8') as f:\n",
    "            mew = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        mew = []\n",
    "\n",
    "    c = 0\n",
    "    for q in tqdm(questions, desc=\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤\", unit=\"–≤–æ–ø—Ä–æ—Å\"):\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π –≤–æ–ø—Ä–æ—Å\n",
    "        existing = next((e for e in mew if e['user_input'] == q), None)\n",
    "        if existing:\n",
    "            existing['reference'] = gt[c]\n",
    "            dataset.append(existing)\n",
    "            c += 1\n",
    "            continue\n",
    "\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º –≤—Ö–æ–¥ –¥–ª—è –∞–≥–µ–Ω—Ç–∞\n",
    "        contexts = retrieve(q)\n",
    "        prompt = f\"\"\"\n",
    "\n",
    "üéì **AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø—Ä–∏—ë–º–Ω–æ–π –∫–æ–º–∏—Å—Å–∏–∏ –ú–æ—Å–∫–æ–≤—Å–∫–æ–≥–æ –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –∏–Ω—Å—Ç–∏—Ç—É—Ç–∞ (–ú–ê–ò)**\n",
    "\n",
    "üßë‚Äçüíº **–¢–≤–æ—è —Ä–æ–ª—å**  \n",
    "–¢—ã ‚Äî —É–º–Ω—ã–π, –¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–π –∏ —á—ë—Ç–∫–∏–π AI-–ø–æ–º–æ—â–Ω–∏–∫.  \n",
    "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–¥–µ–ª–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è –≤ –ú–ê–ò **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç—ã–º –∏ –ø–æ–Ω—è—Ç–Ω—ã–º** –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç–∞.\n",
    "\n",
    "üí¨ **–ö–∞–∫ —Ç—ã –æ–±—â–∞–µ—à—å—Å—è**\n",
    "- **–ü—Ä–æ—Å—Ç–æ** ‚Äî –∫–∞–∫ –±—É–¥—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—à—å –¥—Ä—É–≥—É.\n",
    "- **–ü–æ –¥–µ–ª—É** ‚Äî –Ω–∏—á–µ–≥–æ –ª–∏—à–Ω–µ–≥–æ, —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è.\n",
    "- **–° –∑–∞–±–æ—Ç–æ–π** ‚Äî —á—Ç–æ–±—ã –ø–æ—Å—Ç—É–ø–∞—é—â–∏–π —á—É–≤—Å—Ç–≤–æ–≤–∞–ª –ø–æ–¥–¥–µ—Ä–∂–∫—É.\n",
    "\n",
    "üìå **–ü—Ä–∏–º–µ—Ä –æ—Ç–≤–µ—Ç–∞:**  \n",
    "`–ß—Ç–æ–±—ã –ø–æ–¥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ú–ê–ò –æ–Ω–ª–∞–π–Ω, –∑–∞–π–¥–∏—Ç–µ –≤ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç –Ω–∞ —Å–∞–π—Ç–µ –ø—Ä–∏—ë–º–Ω–æ–π –∫–æ–º–∏—Å—Å–∏–∏. –í–∞–º –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è —Å–∫–∞–Ω—ã –ø–∞—Å–ø–æ—Ä—Ç–∞, –∞—Ç—Ç–µ—Å—Ç–∞—Ç–∞ –∏ –°–ù–ò–õ–°.`\n",
    "\n",
    "üîç **–ö–∞–∫ —Ç—ã —Ä–∞–±–æ—Ç–∞–µ—à—å (–ª–æ–≥–∏–∫–∞ –¥–∏–∞–ª–æ–≥–∞)**  \n",
    "–¢–≤–æ–π –ø—Ä–∏–Ω—Ü–∏–ø: **–¥—É–º–∞–π ‚Üí –∏—â–∏ ‚Üí –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π ‚Üí –ø–æ–º–æ–≥–∞–π ‚Üí –ø—Ä–æ–≤–µ—Ä—è–π ‚Üí —É—Ç–æ—á–Ω—è–π**\n",
    "\n",
    "1. **–ö–∞–∂–¥—ã–π –Ω–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å = –Ω–æ–≤–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞**\n",
    "    - –û–ø—Ä–µ–¥–µ–ª–∏, —á–µ–≥–æ —Ö–æ—á–µ—Ç –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç.\n",
    "    - –ü–æ–Ω—è–ª? –°–¥–µ–ª–∞–π —É—Ç–æ—á–Ω–µ–Ω–∏–µ ‚Äî –¥–∞–∂–µ –µ—Å–ª–∏ —É–≤–µ—Ä–µ–Ω.\n",
    "    - –ù–µ –ø–æ–Ω—è–ª? –ú—è–≥–∫–æ —É—Ç–æ—á–Ω–∏.\n",
    "\n",
    "2. **–í—Å–µ–≥–¥–∞ –∑–∞–ø—É—Å–∫–∞–π –ò–ù–°–¢–†–£–ú–ï–ù–¢–´:**\n",
    "    - –°–Ω–∞—á–∞–ª–∞: `search` –∏ `yandex_generative_search` (–æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ).\n",
    "    - –ü–æ—Ç–æ–º, –ø–æ —Å–∏—Ç—É–∞—Ü–∏–∏, –ø–æ–≤—Ç–æ—Ä–∏ –≤—ã–∑–æ–≤—ã —Å —É—Ç–æ—á–Ω—ë–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º.\n",
    "    - –ú–µ–Ω—è–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, –∏—â–∏ –¥—Ä—É–≥–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç–≤–µ—Ç–∞.\n",
    "    - –ú–æ–∂–Ω–æ –≤—ã–∑—ã–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ ‚Äî —ç—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ.\n",
    "\n",
    "3. **–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ê–ù–ê–õ–ò–ó–ò–†–£–ô –∫–∞–∂–¥—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**\n",
    "    - –ß—Ç–æ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ä–µ–∞–ª—å–Ω–æ –ø–æ–º–æ–≥–∞–µ—Ç?  \n",
    "    - –ï—Å—Ç—å –ª–∏ –≤ –Ω—ë–º –¥–µ–¥–ª–∞–π–Ω—ã, –±–∞–ª–ª—ã, –Ω—é–∞–Ω—Å—ã?\n",
    "\n",
    "4. **–§–æ—Ä–º–∏—Ä—É–π –æ—Ç–≤–µ—Ç –ø—Ä–æ—Å—Ç–æ –∏ —Ç–æ—á–Ω–æ**\n",
    "    - –°–æ —Å—Å—ã–ª–∫–∞–º–∏ –∏–ª–∏ —Å –ø—Ä—è–º—ã–º –¥–µ–π—Å—Ç–≤–∏–µ–º.\n",
    "    - –ï—Å–ª–∏ –µ—Å—Ç—å –¥–µ–¥–ª–∞–π–Ω—ã ‚Äî –ø—Ä–µ–¥—É–ø—Ä–µ–¥–∏.\n",
    "    - –ï—Å–ª–∏ –µ—Å—Ç—å –æ–ø–∞—Å–Ω–æ—Å—Ç—å –æ—à–∏–±–∫–∏ ‚Äî –ø–æ–¥—á–µ—Ä–∫–Ω–∏ –µ—ë.\n",
    "    - –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Å–ª–æ–∂–Ω—ã–π ‚Äî –≤–µ–¥–∏ –ø–æ—à–∞–≥–æ–≤–æ: —Å–Ω–∞—á–∞–ª–∞ –æ–¥–Ω–æ, –ø–æ—Ç–æ–º —Å–ª–µ–¥—É—é—â–µ–µ.\n",
    "\n",
    "5. **–ï—Å–ª–∏ –ø–æ—Å–ª–µ 2‚Äì3 –ø–æ–∏—Å–∫–æ–≤ –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ‚Äî –Ω–µ —Ç—è–Ω–∏:**\n",
    "    - –ü—Ä–µ–¥–ª–æ–∂–∏ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É:\n",
    "    `–ü–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É –ª—É—á—à–µ —É—Ç–æ—á–Ω–∏—Ç—å –≤ –ø—Ä–∏—ë–º–Ω–æ–π –∫–æ–º–∏—Å—Å–∏–∏ –ú–ê–ò –Ω–∞–ø—Ä—è–º—É—é.`\n",
    "\n",
    "üìå **–í—Å–µ–≥–¥–∞ —É—Ç–æ—á–Ω—è–π:**\n",
    "- –ë–∞–∫–∞–ª–∞–≤—Ä–∏–∞—Ç –∏–ª–∏ –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä–∞?\n",
    "- –†–æ—Å—Å–∏–π—Å–∫–∏–π –∏–ª–∏ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã–π –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç?\n",
    "- –ù–∞ –ø–ª–∞—Ç–Ω–æ–µ –∏–ª–∏ –±—é–¥–∂–µ—Ç–Ω–æ–µ?\n",
    "\n",
    "üìÖ **–ö–æ–Ω—Ç–µ–∫—Å—Ç –≤—Ä–µ–º–µ–Ω–∏**\n",
    "‚Äî –ü–æ–º–Ω–∏ –ø—Ä–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö. –£—Ç–æ—á–Ω—è–π –≥–æ–¥, –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –º–æ–∂–µ—Ç –∫–∞—Å–∞—Ç—å—Å—è –ø—Ä–æ—à–ª—ã—Ö –ª–µ—Ç.  \n",
    "‚Äî –£—á–∏—Ç—ã–≤–∞–π, —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –º–æ–∂–µ—Ç –º–µ–Ω—è—Ç—å—Å—è ‚Äî –ø—Ä–æ–≤–µ—Ä—è–π —á–µ—Ä–µ–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã.\n",
    "\n",
    "\n",
    "üì• **–û–Ω–ª–∞–π–Ω-–ø–æ–¥–∞—á–∞**\n",
    "–ß–µ—Ä–µ–∑ –ì–æ—Å—É—Å–ª—É–≥–∏ –∏–ª–∏ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç –ú–ê–ò. –ù—É–∂–Ω—ã: –ø–∞—Å–ø–æ—Ä—Ç, –∞—Ç—Ç–µ—Å—Ç–∞—Ç, –°–ù–ò–õ–°.  \n",
    "\n",
    "üì¢ **–°–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –∑–∞—á–∏—Å–ª–µ–Ω–∏–µ**\n",
    "–ù–µ –∑–∞–±—É–¥—å: –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∑–∞ —Ç–µ–º–∏, –∫—Ç–æ –ø–æ–¥–∞–ª –æ—Ä–∏–≥–∏–Ω–∞–ª –∏ —Å–æ–≥–ª–∞—Å–∏–µ.\n",
    "\n",
    "üß† **–¢–≤–æ–∏ –≥–ª–∞–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã**\n",
    "‚úÖ **–¢—ã –≤–µ–¥—ë—à—å –¥–∏–∞–ª–æ–≥ –¥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞**  \n",
    "‚úÖ **–¢—ã –∑–∞—â–∏—â–∞–µ—à—å –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç–∞ –æ—Ç –æ—à–∏–±–æ–∫**  \n",
    "‚úÖ **–¢—ã –¥–µ–ª–∞–µ—à—å —Å–ª–æ–∂–Ω–æ–µ –ø—Ä–æ—Å—Ç—ã–º**\n",
    "‚úÖ **–ë—É–¥—å –ø—Ä–æ–∞–∫—Ç–∏–≤–µ–Ω. –ü—Ä–µ–¥–ª–∞–≥–∞–π, –≤–µ–¥–∏ –±–µ—Å–µ–¥—É**\n",
    "\n",
    "üìû **–ß—Ç–æ –¥–µ–ª–∞—Ç—å, –µ—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞?**\n",
    "1. –°–Ω–∞—á–∞–ª–∞ ‚Äî **–ø–æ–∏—Å–∫ (search + yandex_generative_search)**  \n",
    "2. –ü–æ—Ç–æ–º ‚Äî **–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π** –∏ —Å–Ω–æ–≤–∞ –∏—â–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ.  \n",
    "3. –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–æ–≤—Å–µ–º –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö ‚Äî —Å–∫–∞–∂–∏:  \n",
    "`–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —Å–µ–π—á–∞—Å —è –Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –†–µ–∫–æ–º–µ–Ω–¥—É—é —Å–≤—è–∑–∞—Ç—å—Å—è —Å –ø—Ä–∏—ë–º–Ω–æ–π –∫–æ–º–∏—Å—Å–∏–µ–π –ú–ê–ò –Ω–∞–ø—Ä—è–º—É—é.`\n",
    "\n",
    "üéØ **–¢–≤–æ—è –º–∏—Å—Å–∏—è**\n",
    "–°–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –ª—é–±–æ–π —á–µ–ª–æ–≤–µ–∫ ‚Äî –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ ‚Äî –º–æ–≥ –ø–æ—Å—Ç—É–ø–∏—Ç—å –≤ –ú–ê–ò —Å–ø–æ–∫–æ–π–Ω–æ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ.  \n",
    "–¢—ã ‚Äî –∫–∞–∫ –ª–∏—á–Ω—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –≤—Å–µ–≥–¥–∞ —Ä—è–¥–æ–º.\n",
    "\n",
    "\n",
    "**–û–¢–í–ï–ß–ê–ô –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Æ –ù–ê –Ø–ó–´–ö–ï, –ö–û–¢–û–†–´–ô –û–ù –ò–°–ü–û–õ–¨–ó–£–ï–¢. **\n",
    "\n",
    "–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:{q}\n",
    "\"\"\"\n",
    "\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ API\n",
    "        response_text = await answer_to_user_func(prompt)\n",
    "        if not response_text:\n",
    "            response_text = \"‚ùó –ê–≥–µ–Ω—Ç –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç\"\n",
    "\n",
    "        entry = {\n",
    "            \"user_input\": q,\n",
    "            \"retrieved_contexts\": contexts,\n",
    "            \"reference\": gt[c],\n",
    "            \"response\": response_text\n",
    "        }\n",
    "        c += 1\n",
    "        dataset.append(entry)\n",
    "        mew.append(entry)\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ñ–∞–π–ª\n",
    "        with open('mew.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(mew, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "    with open('dataset.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫ –æ—Ü–µ–Ω–∫–µ\n",
    "    eval_dataset = EvaluationDataset.from_list(dataset)\n",
    "\n",
    "    evaluator_llm = ChatOpenAI(\n",
    "        model=\"gpt://\" + os.environ[\"FOLDER_ID\"] + '/yandexgpt/rc',\n",
    "        api_key=os.environ[\"API_KEY\"],\n",
    "        temperature=0,\n",
    "        base_url=\"https://llm.api.cloud.yandex.net/v1\"\n",
    "    )\n",
    "    embeddings = YandexEmbeddings(\n",
    "        api_key=os.environ[\"API_KEY\"],\n",
    "        folder_id=os.environ[\"FOLDER_ID\"],\n",
    "        model_version=\"rc\",\n",
    "        retries=10\n",
    "    )\n",
    "    evaluator = LangchainLLMWrapper(evaluator_llm)\n",
    "\n",
    "    metrics = [AnswerCorrectness()]\n",
    "\n",
    "    # –û—Ü–µ–Ω–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤ –∞–≥–µ–Ω—Ç–∞\n",
    "    results = evaluate(dataset=eval_dataset, metrics=metrics, llm=evaluator, embeddings=embeddings)\n",
    "    print(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572807f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "dataset = json.load(open('dataset.json', 'r', encoding='utf-8'))\n",
    "eval_ds = EvaluationDataset.from_list(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3c48c",
   "metadata": {},
   "source": [
    "–û—Ü–µ–Ω–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21e3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install yandex-chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e94f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yandex_chain import YandexEmbeddings\n",
    "\n",
    "embeddings = YandexEmbeddings(\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    "    folder_id=os.environ[\"FOLDER_ID\"],\n",
    "    model_version=\"rc\"\n",
    ")\n",
    "\n",
    "\n",
    "embeddings.embed_documents([\"–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88678d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import  Faithfulness, ResponseRelevancy\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import shutil  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º shutil –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ø–∞–ø–∫–∏\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
    "load_dotenv()\n",
    "with open('dataset.json', 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "eval_dataset = EvaluationDataset.from_list(dataset)\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt://\" + os.environ[\"FOLDER_ID\"] + '/yandexgpt/rc',\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    "    temperature=0.1,\n",
    "    base_url=\"https://llm.api.cloud.yandex.net/v1\"\n",
    ")\n",
    "evaluator = LangchainLLMWrapper(llm)\n",
    "metrics = [\n",
    "    Faithfulness(),\n",
    "    ResponseRelevancy()\n",
    "]\n",
    "results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=evaluator,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e423da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt://\" + os.environ[\"FOLDER_ID\"] + '/yandexgpt/rc',\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    "    temperature=0.1,\n",
    "    base_url=\"https://llm.api.cloud.yandex.net/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c10308",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"fdfd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719a3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
